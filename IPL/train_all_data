import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, random_split
import pandas as pd
import ast
from model import MLP 


class CustomLoss(nn.Module):
    def __init__(self):
        super(CustomLoss, self).__init__()

    def forward(self, outputs, labels):
        loss_func = nn.MSELoss()  
        loss = loss_func(outputs, labels)  
        
        # p2와 p3의 합이 1을 초과하는 경우에 대한 페널티 계산
        sum_p2_p3 = torch.sum(outputs[:, 0] + outputs[:, 1])
        #penalty_p2_p3 = torch.relu(sum_p2_p3 - 1.0)  # 1을 초과하는 경우에만 페널티 적용
        #loss += penalty_p2_p3

        # 각 출력 값에 대해 해당하는 패널티를 비례적으로 계산하여 손실에 추가
        for i in range(1, 3):
            penalty_p2_p3_i = torch.relu(outputs[:, i] - 1.0) * outputs[:, i]  
            loss += penalty_p2_p3_i.sum()  

        # p5부터 p8까지의 합이 1을 초과하는 경우에 대한 페널티 계산
        sum_p5_to_p8 = torch.sum(outputs[:, 4:], dim=1)
        penalty_p5_to_p8 = torch.mean(torch.relu(sum_p5_to_p8 - 1.0))#1을 초과하는 경우에만 패널티 적용
        # 각 출력 값에 대해 해당하는 패널티를 계산하여 손실에 추가
        for i in range(4, 8):
            penalty_p5_to_p8_i = torch.relu(outputs[:, i] - 1.0) * outputs[:, i]  
            loss += penalty_p5_to_p8_i.sum()   

                
        return loss

if __name__ == "__main__":
    # 데이터프레임 로드
    df = pd.read_csv('P1_toP8.csv')
    
    features = torch.tensor([ast.literal_eval(observed)[:15] for observed in df['subpopulation_values']], dtype=torch.float32)
    labels = torch.tensor(df[['p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8']].values, dtype=torch.float32)  

    dataset = TensorDataset(features, labels)

    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=64)

    model = MLP()

    criterion = CustomLoss()  
    optimizer = optim.Adam(model.parameters(), lr=0.01)

    # 모델 훈련
    num_epochs = 600
    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        for inputs, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)  
            loss.backward()  
            optimizer.step()  
            running_loss += loss.item() * inputs.size(0)
        
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for inputs, labels in val_loader:
                outputs = model(inputs)
                loss = criterion(outputs, labels) 
                val_loss += loss.item() * inputs.size(0)
        
        if (epoch + 1) % 50 == 0:
            print(f'Epoch [{epoch + 1}/{num_epochs}], '
                  f'Training Loss: {running_loss / len(train_dataset):.4f}, '
                  f'Validation Loss: {val_loss / len(val_dataset):.4f}')

    print('Finished Training')



    test_df = pd.read_csv('test_data_all.csv')
    test_df['subpopulation_values'] = test_df['subpopulation_values'].apply(ast.literal_eval)

    test_features = torch.tensor(test_df['subpopulation_values'].tolist(), dtype=torch.float32)
    test_labels = torch.tensor(test_df[['p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8']].values, dtype=torch.float32) 

    test_dataset = TensorDataset(test_features, test_labels)
    test_loader = DataLoader(test_dataset, batch_size=64)

    model.eval()
    test_loss = 0.0
    with torch.no_grad():
        for inputs, labels in test_loader:
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            test_loss += loss.item() * inputs.size(0)

    mse = test_loss / len(test_dataset)
    print(f'Test MSE: {mse:.4f}')

