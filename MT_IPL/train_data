import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, random_split
import pandas as pd
import ast
from sklearn.preprocessing import StandardScaler



df = pd.read_csv('MT_1_8.csv')
features = [ast.literal_eval(observed)[:15] for observed in df['subpopulation_values']]
labels = df[['p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8']].values


scaler = StandardScaler()
features = scaler.fit_transform(features)
features = torch.tensor(features, dtype=torch.float32)
labels = torch.tensor(labels, dtype=torch.float32)

dataset = TensorDataset(features, labels)


train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=64)


class MultiTaskMLP(nn.Module):
    def __init__(self):
        super(MultiTaskMLP, self).__init__()
        self.shared_fc1 = nn.Linear(15, 512) 
        self.dropout = nn.Dropout(0.5)  
        
        # 각 태스크별 Task-specific Layer 정의
        self.task_fc2 = nn.ModuleList([nn.Linear(512, 256) for _ in range(8)])
        self.task_fc3 = nn.ModuleList([nn.Linear(256, 128) for _ in range(8)])
        self.task_fc4 = nn.ModuleList([nn.Linear(128, 1) for _ in range(8)])
        
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = F.leaky_relu(self.shared_fc1(x))
        x = self.dropout(x)  
        
        task_outputs = []
        for i in range(8):
            task_x = F.leaky_relu(self.task_fc2[i](x))
            task_x = F.leaky_relu(self.task_fc3[i](task_x))
            task_output = torch.sigmoid(self.task_fc4[i](task_x))
            task_outputs.append(task_output)
        
        return torch.cat(task_outputs, dim=-1)


model = MultiTaskMLP()
criterion = nn.MSELoss()
optimizer = optim.RMSprop(model.parameters(), lr=0.001) 
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1) 


def train_model(model, criterion, optimizer, scheduler, num_epochs=600):
    model.train()
    for epoch in range(num_epochs):
        running_loss = 0.0
        for inputs, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
        scheduler.step() 
        
        if (epoch + 1) % 50 == 0:
            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')


train_model(model, criterion, optimizer, scheduler)

def evaluate_model(data_loader, model):
    model.eval()
    mse_loss = torch.nn.MSELoss()
    total_loss = 0.0
    count = 0
    
    with torch.no_grad():
        for inputs, labels in data_loader:
            preds = model(inputs)
            loss = mse_loss(preds, labels)
            total_loss += loss.item() * inputs.size(0)
            count += inputs.size(0)
    
    return total_loss / count


val_loss = evaluate_model(val_loader, model)
print(f'Validation MSE Loss: {val_loss:.4f}')


test_df = pd.read_csv('test_data_all.csv')
test_df['subpopulation_values'] = test_df['subpopulation_values'].apply(ast.literal_eval)
test_features = scaler.transform(test_df['subpopulation_values'].tolist())
test_features = torch.tensor(test_features, dtype=torch.float32)
test_labels = torch.tensor(test_df[['p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8']].values, dtype=torch.float32)
test_dataset = TensorDataset(test_features, test_labels)
test_loader = DataLoader(test_dataset, batch_size=64)


test_loss = evaluate_model(test_loader, model)
print(f'Test MSE Loss: {test_loss:.4f}')
